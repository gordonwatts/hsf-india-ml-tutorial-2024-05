{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3V3tNoJVYDF"
   },
   "source": [
    "# Hands on : introduction to NN on HEP dataset using JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfxlugZJVlR0"
   },
   "source": [
    "### Many thanks to _Rafael Coelho Lopes De Sa, Fernando Torales Acosta, David Rousseau, Yann Coadou_, and _Aishik Gosh_, and others for help with this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eT7-MMpfrlHR"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FO_W4IvbVYDL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Because we like cool progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import jax\n",
    "\n",
    "%matplotlib inline\n",
    "import time\n",
    "pd.set_option('display.max_columns', None) # to see all columns of df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.28\n"
     ]
    }
   ],
   "source": [
    "print(jax.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX can use GPU's automatically (almost no change to code below). However, for this tutorial we will just use the CPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHSzcuTgVYDT",
    "tags": []
   },
   "source": [
    "# Load events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was created from ATLAS Open Data. Lets load it in! Jim's lectures will show us more of these details. We have a simple function that will load and clean up the data a bit. It will return it in a `pandas.DataFrame`.\n",
    "\n",
    "* Feel free to inspect what it does in the `extra_function.py` file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extra_functions import load_training_file\n",
    "all_data = load_training_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a quick look at a few things, so we can \"see\" what our data looks like. FIrst - how many rows are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567749"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the length of the data\n",
    "len(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the list of columns that are in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'eventNumber', 'label', 'met_et', 'met_phi', 'lep_n',\n",
       "       'lep_pt_0', 'lep_pt_1', 'lep_eta_0', 'lep_eta_1', 'lep_phi_0',\n",
       "       'lep_phi_1', 'lep_E_0', 'lep_E_1', 'lep_charge_0', 'lep_charge_1',\n",
       "       'lep_type_0', 'lep_type_1', 'jet_n', 'jet_pt_0', 'jet_pt_1',\n",
       "       'jet_eta_0', 'jet_eta_1', 'jet_phi_0', 'jet_phi_1', 'jet_E_0',\n",
       "       'jet_E_1', 'mcWeight', 'runNumber', 'channelNumber'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of columns\n",
    "all_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we have lep 0 and 1, and jet 0 and jet 1. But they are unrolled (not, as you will learn, an `awkward` array)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `label` column tells us if it is signal (`1`) or background (`0`). And we can make some plots. You'll learn more about plotting later in the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print out the length of signal and background - to make sure we have enough to test of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390398, 177351)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of signal and background\n",
    "len(all_data[all_data['label']==1]), len(all_data[all_data['label']==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more very important prep step is to *shuffle* the data before we use it.\n",
    "\n",
    "* If you train on sub-samples this assures there is a good mix.\n",
    "* Files are often built by putting signal first and background second - meaning all the events some in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data = all_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now - lets look at the data and see, visually, how it looks. You'll learn later this week how to use `matplotlib` - the code below is very crude, but it looks at all the variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "ax=all_data[all_data.label==0].hist(weights=all_data.mcWeight[all_data.label==0],figsize=(15,12),color='b',alpha=0.5,density=True,bins=50,grid=False)\n",
    "ax=ax.flatten()[:all_data.shape[1]] # to avoid error if holes in the grid of plots (like if 7 or 8 features)\n",
    "all_data[all_data.label==1].hist(weights=all_data.mcWeight[all_data.label==1],figsize=(15,12),color='r',alpha=0.5,density=True,ax=ax,bins=50,grid=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What look like good variables? Do they make physics sense?\n",
    "\n",
    "What variables make no physics sense to train on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that the phi's aren't that different (as expected due to the symmetry of the beamline/detector) So, lets start with a safe set. You can come back later and modify this list if you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with a sub-set of columns to make this easy:  `[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_phi_0', 'lep_phi_1']`\n",
    "\n",
    "Create the variable `data` in the next cell which only has those columns from `all_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just use the columns we want to train against.\n",
    "data = all_data.loc[:, [\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_phi_0', 'lep_phi_1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Krg3pTRjVYDs",
    "tags": []
   },
   "source": [
    "#### Feature engineering\n",
    "\n",
    "Besides adding in variables like above, we can also create new variables using our physics knowledge. For example, we know that the open angle between the two leptons can be a good discriminator. The NN might be able to learn this - but since we know, we might as well help it out.\n",
    "\n",
    "We'll leave this protected for now so that you can come back and try this out later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DH5UqBF9VYDt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_delta_phi = False\n",
    "if use_delta_phi: \n",
    "    data[\"lep_deltaphi\"]=np.abs(np.mod(data.lep_phi_1-data.lep_phi_0+3*np.pi,2*np.pi)-np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a quick look at the variables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>met_et</th>\n",
       "      <th>met_phi</th>\n",
       "      <th>lep_pt_0</th>\n",
       "      <th>lep_pt_1</th>\n",
       "      <th>lep_phi_0</th>\n",
       "      <th>lep_phi_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47.684</td>\n",
       "      <td>-0.77197</td>\n",
       "      <td>37.166</td>\n",
       "      <td>28.3090</td>\n",
       "      <td>-2.71730</td>\n",
       "      <td>-1.44840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.215</td>\n",
       "      <td>-0.20048</td>\n",
       "      <td>106.710</td>\n",
       "      <td>22.0740</td>\n",
       "      <td>-0.17410</td>\n",
       "      <td>2.15690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42.411</td>\n",
       "      <td>1.17200</td>\n",
       "      <td>27.401</td>\n",
       "      <td>15.5490</td>\n",
       "      <td>-0.85048</td>\n",
       "      <td>-1.46880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34.865</td>\n",
       "      <td>0.77953</td>\n",
       "      <td>40.732</td>\n",
       "      <td>7.4211</td>\n",
       "      <td>-3.12120</td>\n",
       "      <td>-0.68055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.508</td>\n",
       "      <td>-0.61760</td>\n",
       "      <td>73.652</td>\n",
       "      <td>35.7690</td>\n",
       "      <td>0.29708</td>\n",
       "      <td>2.67520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   met_et  met_phi  lep_pt_0  lep_pt_1  lep_phi_0  lep_phi_1\n",
       "0  47.684 -0.77197    37.166   28.3090   -2.71730   -1.44840\n",
       "1  92.215 -0.20048   106.710   22.0740   -0.17410    2.15690\n",
       "2  42.411  1.17200    27.401   15.5490   -0.85048   -1.46880\n",
       "3  34.865  0.77953    40.732    7.4211   -3.12120   -0.68055\n",
       "4  46.508 -0.61760    73.652   35.7690    0.29708    2.67520"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kowHjX4rlIC"
   },
   "source": [
    "# Training & Testing Samples\n",
    "\n",
    "First we need to split the data into test and training samples. `scikit-learn` has some great utilities that make this a breeze.\n",
    "\n",
    "First thing to consider - what fraction do we want to use for training vs testing? Make this a small number to speed training (e.g. small number of samples to train on) and a larger number for more accurate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally `X` is the data we train on, `Y` is _ground truth_ - what we are aiming for, and `weights` are per event weights (usually from MC).\n",
    "\n",
    "We also split things into `test` and `train` for testing and training. The training sample will be biased by the training, of course, so we keep the `test` sample independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (567749, 6), Y shape: (567749,), weights shape: (567749,)\n"
     ]
    }
   ],
   "source": [
    "X = data\n",
    "y = all_data.label\n",
    "weights = all_data.mcWeight\n",
    "\n",
    "print(f\"X shape: {X.shape}, Y shape: {y.shape}, weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that everything is the same length! If not, very bad things will happen below!\n",
    "\n",
    "Ok - next we can use a very useful library routine, `train_test_split` to split everything up. Look up on the internet how to use this, and then code up the below to generate `X_train, X_test, y_train, y_test, weights_train, weights_test` using the fraction `train_size`. Finally, print out everyone's shape so that we can make sure we aren't making an obvious mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9j5hdrmrlID",
    "outputId": "75c0925f-74c9-4e24-d169-8a884b510f4c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56774, 6) (510975, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(X, y, weights, train_size=train_size)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, when doing a real ML training, you'll want to split the test dataset in half - for a test and validation datasets:\n",
    "\n",
    "- __Training Dataset:__ The sample of data used to fit the model.\n",
    "- __Validation Dataset:__ The sample used to provide an unbiased evaluation of a model fit on the training dataset while tuning  hyperparameters.\n",
    "- __Test Dataset:__ The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "Why are we so worried about bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to the JAX data arrays\n",
    "\n",
    "JAX has its own data array types. This is because it wants to be able to work on both your CPU and your GPU - or even remotely. As a result it has a concept of a `DeviceArray` - something `numpy` does not need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "X_train = jnp.array(X_train)\n",
    "X_test = jnp.array(X_test)\n",
    "y_train = jnp.array(y_train)\n",
    "y_test = jnp.array(y_test)\n",
    "weights_train = jnp.array(weights_train)\n",
    "weights_test = jnp.array(weights_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxybCOi-rlIM"
   },
   "source": [
    "# Building a JAX Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets build the JAX NN as we have done in previous efforts. Think a little bit about what we want the output to look like - we want it to be a zero if it is background and a 1 if it is signal. It should never go beyond that.\n",
    "\n",
    "Use the JAX example from earlier today to figure out how to code this up. The final Haiku network should be called `net`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn\n",
    "import haiku as hk\n",
    "from optax import adam, apply_updates\n",
    "def net_fn(x):\n",
    "    mlp = hk.Sequential([\n",
    "    hk.Linear(12), jax.nn.relu,\n",
    "    hk.Linear(60), jax.nn.relu,\n",
    "    hk.Linear(32), jax.nn.relu,\n",
    "    hk.Linear(1), jax.nn.sigmoid\n",
    "    ])\n",
    "    return mlp(x)\n",
    "net = hk.transform(net_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, randomly initalize the parameters we'll be training on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(42)\n",
    "params = net.init(rng, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer we'll use - default to `adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "optimizer = optax.adam(0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the loss function. Lets use the same one we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optax import sigmoid_binary_cross_entropy\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(params, x, y):\n",
    "    preds = net.apply(params, rng, x)\n",
    "    # This next line provides a x10 speed up.\n",
    "    preds = preds.reshape(-1)\n",
    "    sm_array = sigmoid_binary_cross_entropy(preds, y)\n",
    "    return jnp.mean(sm_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the update function that will generate an update of the code. Note that we should JIT the `update` function to speed it up. See the demo from this morning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(params, opt_state, x, y):\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = apply_updates(params, updates)\n",
    "    return new_params, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state = optimizer.init(params)\n",
    "\n",
    "losses_training = []\n",
    "losses_test = []\n",
    "\n",
    "# How much of the training sample to calculate the loss on.\n",
    "# Keep it large enough to be meaningful, but small enough to be fast.\n",
    "n_loss = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop, finally! Note that we've designed this so that we can keep re-running it... so if the first 10 aren't enough, we can just re-run the loop. Write a training loop:\n",
    "\n",
    "* Uses `tqdm` to run over 1000 epochs\n",
    "* runs a `train_step` each one.\n",
    "* Calculates the loss on the test and training sample, and adds them to `losses_training` and `losses_test`\n",
    "* We should be able to run the cell multiple times without it erasing prior work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets plot the training losses to see how well we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses_training,label=\"training loss\")\n",
    "plt.plot(losses_test,label=\"test loss\")\n",
    "\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Training\n",
    "\n",
    "Evaluate the model based on predictions made with X_test $\\rightarrow$ y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_test = net.apply(params, rng, X_test)\n",
    "y_pred_train = net.apply(params, rng, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ROC curves and Area Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfB3mkkxW4CP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
    "from sklearn.utils import class_weight # to set class_weight=\"balanced\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr,tpr,_ = roc_curve(y_true=y_test, y_score=y_pred_test,sample_weight=weights_test)\n",
    "plt.plot(fpr, tpr, color='blue',lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auc_test = roc_auc_score(y_true=y_test, y_score=y_pred_test,sample_weight=weights_test)\n",
    "auc_train = roc_auc_score(y_true=y_train, y_score=y_pred_train,sample_weight=weights_train)\n",
    "print(\"auc test:\",auc_test)\n",
    "print (\"auc train:\",auc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting NN Score for Signal and Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from extra_functions import compare_train_test\n",
    "compare_train_test(y_pred_train.reshape(-1), y_train, y_pred_test.reshape(-1), y_test, \n",
    "                   xlabel=\"NN Score\", title=\"NN\", \n",
    "                   weights_train=weights_train, weights_test=weights_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HEPML_HandsOn_NN_veryold.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
