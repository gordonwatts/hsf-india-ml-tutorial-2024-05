{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3V3tNoJVYDF"
   },
   "source": [
    "# Hands on : introduction to NN on HEP dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfxlugZJVlR0"
   },
   "source": [
    "### Many thanks to _Rafael Coelho Lopes De Sa, Fernando Torales Acosta, David Rousseau, Yann Coadou_, and _Aishik Gosh_, and others for help with this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eT7-MMpfrlHR"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FO_W4IvbVYDL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import jax\n",
    "\n",
    "%matplotlib inline\n",
    "import time\n",
    "pd.set_option('display.max_columns', None) # to see all columns of df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(jax.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX can use GPU's automatically (almost no change to code below). However, for this tutorial we will just use the CPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHSzcuTgVYDT",
    "tags": []
   },
   "source": [
    "# Load events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was created from ATLAS Open Data. Lets load it in! Jim's lectures will show us more of these details. We have a simple function that will load and clean up the data a bit. It will return it in a `pandas.DataFrame`.\n",
    "\n",
    "* Feel free to inspect what it does in the `extra_function.py` file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extra_functions import load_training_file\n",
    "all_data = load_training_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a quick look at a few things, so we can see what we are looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kl-4W4Ifs8EV",
    "outputId": "54d366bc-8925-4c6a-e867-ef70016e016b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "f\"{len(all_data):,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we have lep 0 and 1, and jet 0 and jet 1. But they are unrolled (not, as you will learn, an `awkward` array)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `label` column tells us if it is signal (`1`) or background (`0`). And we can make some plots. You'll learn more about plotting later in the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more very important prep step is to *shuffle* the data before we use it.\n",
    "\n",
    "* If you train on sub-samples this assures there is a good mix.\n",
    "* Files are often built by putting signal first and background second - meaning all the events some in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data = all_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now - lets look at the data and see, visually, how it looks. You'll learn later this week how to use `matplotlib` - the code below is very crude, but it looks at all the variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "ax=all_data[all_data.label==0].hist(weights=all_data.mcWeight[all_data.label==0],figsize=(15,12),color='b',alpha=0.5,density=True,bins=50,grid=False)\n",
    "ax=ax.flatten()[:all_data.shape[1]] # to avoid error if holes in the grid of plots (like if 7 or 8 features)\n",
    "all_data[all_data.label==1].hist(weights=all_data.mcWeight[all_data.label==1],figsize=(15,12),color='r',alpha=0.5,density=True,ax=ax,bins=50,grid=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there are a number of items that should not be used for training. Like `runNumber` or `label` (which is ground truth for the training!!!). Also note that the phi's aren't that different (as expected due to the symmetry of the beamline/detector) So, lets start with a safe set. You can come back later and modify this list if you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_train_on = [\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_phi_0', 'lep_phi_1']\n",
    "data=all_data.loc[:, columns_to_train_on]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Krg3pTRjVYDs",
    "tags": []
   },
   "source": [
    "#### Feature engineering\n",
    "\n",
    "Besides adding in variables like above, we can also create new variables using our physics knowledge. For example, we know that the open angle between the two leptons can be a good discriminator. The NN might be able to learn this - but since we know, we might as well help it out.\n",
    "\n",
    "We'll leave this protected for now so that you can come back and try this out later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DH5UqBF9VYDt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_delta_phi = False\n",
    "if use_delta_phi: \n",
    "    data[\"lep_deltaphi\"]=np.abs(np.mod(data.lep_phi_1-data.lep_phi_0+3*np.pi,2*np.pi)-np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kowHjX4rlIC"
   },
   "source": [
    "# Preparing the data for training.\n",
    "\n",
    "First we need to split the data into test and training samples. `scikit-learn` has some great utilities that make this a breeze.\n",
    "\n",
    "First thing to consider - what fraction do we want to use for training vs testing? Make this a small number to speed training (e.g. small number of samples to train on) and a larger number for more accurate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally `X` is the data we train on, `Y` is _ground truth_ - what we are aiming for, and `weights` are per event weights (usually from MC).\n",
    "\n",
    "We also split things into `test` and `train` for testing and training. The training sample will be biased by the training, of course, so we keep the `test` sample independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data\n",
    "y = all_data.label\n",
    "weights = all_data.mcWeight\n",
    "\n",
    "print(f\"X shape: {X.shape}, Y shape: {y.shape}, weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that everything is the same length! If not, very bad things will happen below!\n",
    "\n",
    "Ok - next we can use a very useful library routine, `train_test_split` to split everything up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9j5hdrmrlID",
    "outputId": "75c0925f-74c9-4e24-d169-8a884b510f4c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(\n",
    "    X, y, weights, train_size=train_size\n",
    ")\n",
    "\n",
    "print(\"Xtrain Shape: \", X_train.shape)\n",
    "print(\"ytrain Shape: \", y_train.shape)\n",
    "print(\"Training Weights: \", weights_train.shape, \"\\n\")\n",
    "print(\"Xtest Shape: \", X_test.shape)\n",
    "print(\"ytest Shape: \", y_test.shape)\n",
    "print(\"Test Weights: \", weights_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, when doing a real ML training, you'll want to split the test dataset in half - for a test and validation datasets:\n",
    "\n",
    "- __Training Dataset:__ The sample of data used to fit the model.\n",
    "- __Validation Dataset:__ The sample used to provide an unbiased evaluation of a model fit on the training dataset while tuning  hyperparameters.\n",
    "- __Test Dataset:__ The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "Why are we so worried about bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to the JAX data arrays\n",
    "\n",
    "JAX has its own data array types. This is because it wants to be able to work on both your CPU and your GPU - or even remotely. As a result it has a concept of a `DeviceArray` - something `numpy` does not need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "X_train = jnp.array(X_train)\n",
    "X_test = jnp.array(X_test)\n",
    "y_train = jnp.array(y_train)\n",
    "y_test = jnp.array(y_test)\n",
    "weights_train = jnp.array(weights_train)\n",
    "weights_test = jnp.array(weights_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxybCOi-rlIM"
   },
   "source": [
    "# Building a JAX Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets build the JAX NN as we have done in previous efforts. Think a little bit about what we want the output to look like - we want it to be a zero if it is background and a 1 if it is signal. It should never go beyond that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn\n",
    "import haiku as hk\n",
    "from optax import adam, apply_updates\n",
    "\n",
    "def net_fn(x):\n",
    "    mlp = hk.Sequential([\n",
    "        hk.Linear(12), jax.nn.relu,\n",
    "        hk.Linear(60), jax.nn.relu,\n",
    "        hk.Linear(32), jax.nn.relu,\n",
    "        hk.Linear(1), jax.nn.sigmoid\n",
    "    ])\n",
    "    return mlp(x)\n",
    "\n",
    "net = hk.transform(net_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, randomly initalize the parameters we'll be training on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(42)\n",
    "params = net.init(rng, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer we'll use - default to `adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = adam(0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the loss function. Lets use the same one we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "# def loss_fn(params, x, y):\n",
    "#     preds = net.apply(params, rng, x)\n",
    "#     return jnp.mean((preds - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optax import sigmoid_binary_cross_entropy\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(params, x, y):\n",
    "    preds = net.apply(params, rng, x)\n",
    "    # This next line provides a x10 speed up.\n",
    "    preds = preds.reshape(-1)\n",
    "    sm_array = sigmoid_binary_cross_entropy(preds, y)\n",
    "    return jnp.mean(sm_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the update function that will generate an update of the code. Note that we should JIT the `update` function to speed it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(params, opt_state, x, y):\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = apply_updates(params, updates)\n",
    "    return new_params, opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "As before lets initialize the state as ready to go. We will also track the losses each step so we can see how they are going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state = optimizer.init(params)\n",
    "\n",
    "losses_training = []\n",
    "losses_test = []\n",
    "\n",
    "# How much of the training sample to calculate the loss on.\n",
    "# Keep it large enough to be meaningful, but small enough to be fast.\n",
    "n_loss = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop, finally! Note that we've designed this so that we can keep re-running it... so if the first 10 aren't enough, we can just re-run the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "starting_time = time.time()\n",
    "\n",
    "print(\"starting training...\")\n",
    "\n",
    "if len(losses_training) == 0:\n",
    "    # Prime the loss arrays.\n",
    "    losses_training.append(loss_fn(params, X_train[:n_loss], y_train[:n_loss]))\n",
    "    losses_test.append(loss_fn(params, X_test[:n_loss], y_test[:n_loss]))\n",
    "\n",
    "for epoch in tqdm(range(1000)):\n",
    "    params, opt_state = train_step(params, opt_state, X_train, y_train)\n",
    "    losses_training.append(loss_fn(params, X_train[:n_loss], y_train[:n_loss]))\n",
    "    losses_test.append(loss_fn(params, X_test[:n_loss], y_test[:n_loss]))\n",
    "\n",
    "print(\"done training...\")\n",
    "\n",
    "training_time = time.time() - starting_time\n",
    "print(\"Training time:\", training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses_training,label=\"training loss\")\n",
    "plt.plot(losses_test,label=\"test loss\")\n",
    "\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Training\n",
    "\n",
    "Evaluate the model based on predictions made with X_test $\\rightarrow$ y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_test = net.apply(params, rng, X_test)\n",
    "y_pred_train = net.apply(params, rng, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ROC curves and Area Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfB3mkkxW4CP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
    "from sklearn.utils import class_weight # to set class_weight=\"balanced\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr,tpr,_ = roc_curve(y_true=y_test, y_score=y_pred_test,sample_weight=weights_test)\n",
    "plt.plot(fpr, tpr, color='blue',lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auc_test = roc_auc_score(y_true=y_test, y_score=y_pred_test,sample_weight=weights_test)\n",
    "auc_train = roc_auc_score(y_true=y_train, y_score=y_pred_train,sample_weight=weights_train)\n",
    "print(\"auc test:\",auc_test)\n",
    "print (\"auc train:\",auc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting NN Score for Signal and Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from extra_functions import compare_train_test\n",
    "compare_train_test(y_pred_train.reshape(-1), y_train, y_pred_test.reshape(-1), y_test, \n",
    "                   xlabel=\"NN Score\", title=\"NN\", \n",
    "                   weights_train=weights_train, weights_test=weights_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HEPML_HandsOn_NN_veryold.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
