{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3V3tNoJVYDF"
   },
   "source": [
    "# Hands on : introduction to NN on HEP dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfxlugZJVlR0"
   },
   "source": [
    "### Many thanks to _Rafael Coelho Lopes De Sa, Fernando Torales Acosta, David Rousseau, Yann Coadou_, and _Aishik Gosh_, and others for help with this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eT7-MMpfrlHR"
   },
   "source": [
    "## Import Packages\n",
    "\n",
    "[This should look familiar!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FO_W4IvbVYDL",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muproot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mur\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF_CPP_MIN_LOG_LEVEL\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, HTML\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import uproot as ur\n",
    "\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline\n",
    "import time\n",
    "pd.set_option('display.max_columns', None) # to see all columns of df.head()\n",
    "np.random.seed(31415) # set the random seed for the reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are be no GPU's available on this binder hub configuration for this tutorial - but TF (and other frameworks) will automatically switch to using a GPU if they can find it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHSzcuTgVYDT",
    "tags": []
   },
   "source": [
    "# Load events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data was created from ATLAS Open Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kl-4W4Ifs8EV",
    "outputId": "54d366bc-8925-4c6a-e867-ef70016e016b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename=(\"dataWW_d1.root\")\n",
    "file = ur.open(filename)\n",
    "print(file.classnames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree = file[\"tree_event\"]\n",
    "print(tree.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(type(tree))\n",
    "tree.show()\n",
    "dfall = tree.arrays(library=\"pd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#shuffle the events [already done but just to be safe]\n",
    "dfall = dfall.sample(frac=1).reset_index(drop=True)\n",
    "print (\"File loaded with \",dfall.shape[0], \" events \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xYM7O_zVYDY"
   },
   "source": [
    "#### At this point, you should see \"File Loaded with 600000 events\". If not, the data file could not be accessed. No point going further!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFPm8OHuVYDY"
   },
   "source": [
    "# Examine Pandas Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xNeCAIuFVYDZ",
    "outputId": "b12cbd30-536e-429c-a1be-677ee0b937fb"
   },
   "outputs": [],
   "source": [
    "#dump list of feature\n",
    "dfall.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "j9l7wkkorlHe",
    "outputId": "7d824317-2287-44b0-e153-112ecf7ddc13",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#examine first few events\n",
    "dfall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "Oz-lWJhgrlHg",
    "outputId": "7ad3adfa-9819-47cb-c321-6d0d884c96cf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#take a look at feature distribution\n",
    "dfall.describe()\n",
    "#dfall.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQsalTmorlHj",
    "outputId": "58b3c0c9-93c6-4757-cf69-66278940e60f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_weights = (dfall[dfall.label==0].mcWeight.sum(), dfall[dfall.label==1].mcWeight.sum() ) \n",
    "print(\"total label weights\",label_weights)\n",
    "\n",
    "label_nevents = (dfall[dfall.label==0].shape[0], dfall[dfall.label==1].shape[0] )\n",
    "print (\"total class number of events\",label_nevents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtI5u5GErlHq"
   },
   "source": [
    "## Event Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook essentially tries to classify events containing a Higgs Boson.\n",
    "\n",
    "The simulation includes top-quark-pair production, single-top production, production of weak bosons in association with jets (W+jets, Z+jets), production of a pair of bosons (diboson WW, WZ, ZZ) and __SM Higgs__ production.\n",
    "\n",
    "We will only keep events with exactly two leptons dfall.lep_n==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaO2JM1hrlHr",
    "outputId": "bc59e98d-f6f1-42b7-e9f3-48052fe4ebc9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (dfall.shape)\n",
    "\n",
    "# Also only keep events with positive weight. This is in principle wrong. \n",
    "#Many Data Science tools break given a negative weight.\n",
    "\n",
    "fulldata=dfall[(dfall.lep_n==2) & (dfall.mcWeight>0 )] # only keep events with exactly two leptons \n",
    "print (fulldata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iMQpKhDKrlH0",
    "outputId": "ccb36177-999c-4203-939e-4e04383c1954",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hide label and weights in separate vectors\n",
    "#they are not real features\n",
    "\n",
    "#WARNING : there should be no selection nor shuffling later on !\n",
    "target = fulldata[\"label\"]\n",
    "del fulldata[\"label\"]\n",
    "\n",
    "#hide weight in separate vector\n",
    "weights = fulldata[\"mcWeight\"]\n",
    "del fulldata[\"mcWeight\"]\n",
    "fulldata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nviyIMgerlH3"
   },
   "source": [
    "\n",
    "# Try not to change the cells above $\\uparrow$\n",
    "...and return to this cell (or rerun the whole notebook) after changing things below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For simplicity, we'll only use some features on the first pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "6e0Hlpv6rlH4",
    "outputId": "8705b9a0-9dcc-4eb6-ab49-b3fd0b8ea1a6",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data=pd.DataFrame(fulldata, columns=[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_phi_0', 'lep_phi_1'])\n",
    "print (data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Krg3pTRjVYDs",
    "tags": []
   },
   "source": [
    "### Feature engineering (Two variations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. See if using more features improves model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "more_features = False\n",
    "if (more_features):\n",
    "    data=pd.DataFrame(fulldata, columns=[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_eta_0', 'lep_eta_1', 'lep_phi_0', 'lep_phi_1','jet_n','jet_pt_0',\n",
    "       'jet_pt_1', 'jet_eta_0', 'jet_eta_1', 'jet_phi_0', 'jet_phi_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Engineer our own feature, $\\Delta\\varphi_l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DH5UqBF9VYDt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_deltaphi = False\n",
    "if use_deltaphi: \n",
    "    data[\"lep_deltaphi\"]=np.abs(np.mod(data.lep_phi_1-data.lep_phi_0+3*np.pi,2*np.pi)-np.pi)\n",
    "\n",
    "    print (data.shape)\n",
    "    display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "id": "o2mf1bLVrlH7",
    "outputId": "b4484ca0-0983-4477-9af7-26bf496aa618",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "ax=data[target==0].hist(weights=weights[target==0],figsize=(15,12),color='b',alpha=0.5,density=True)\n",
    "ax=ax.flatten()[:data.shape[1]] # to avoid error if holes in the grid of plots (like if 7 or 8 features)\n",
    "data[target==1].hist(weights=weights[target==1],figsize=(15,12),color='r',alpha=0.5,density=True,ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kowHjX4rlIC"
   },
   "source": [
    "# Transform the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data into Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9j5hdrmrlID",
    "outputId": "75c0925f-74c9-4e24-d169-8a884b510f4c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "train_size = 0.1 # fraction of sample used for training\n",
    "# Try changing fraction to increase or decrease sample\n",
    "\n",
    "X_train, X_test, y_train, y_test, weights_train, weights_test = \\\n",
    "    train_test_split(data, target, weights, train_size=train_size)\n",
    "\n",
    "y_train, y_test, weights_train, weights_test = \\\n",
    "    y_train.reset_index(drop=True),y_test.reset_index(drop=True), \\\n",
    "    weights_train.reset_index(drop=True), weights_test.reset_index(drop=True)\n",
    "\n",
    "print (\"Xtrain Shape: \",X_train.shape)\n",
    "print (\"ytrain Shape: \",y_train.shape)\n",
    "print (\"Training Weights: \",weights_train.shape,\"\\n\")\n",
    "print (\"Xtest Shape: \",X_test.shape)\n",
    "print (\"ytest Shape: \",y_test.shape)\n",
    "print (\"Test Weights: \",weights_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing an extra data split. Test _and_ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val, weights_test, weights_val, = \\\n",
    "    train_test_split(X_test, y_test, weights_test, train_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Training Dataset:__ The sample of data used to fit the model.\n",
    "- __Validation Dataset:__ The sample used to provide an unbiased evaluation of a model fit on the training dataset while tuning  hyperparameters.\n",
    "- __Test Dataset:__ The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "Why are we worried about bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the Data\n",
    "\n",
    "**Scale to Mean of 0 and Variance of 1.0:**   $\\ \\ \\ \\ (x-\\mu)/\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test) #applies the transformation calculated the line above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust the Test and Train Signal/Background Weights\n",
    "Train on equal amount of Signal and Background, Test on 'natural' ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_weights_train = (weights_train[y_train == 0].sum(), weights_train[y_train == 1].sum())\n",
    "print (\"class_weights_train:\",class_weights_train)\n",
    "for i in range(len(class_weights_train)):\n",
    "    weights_train[y_train == i] *= max(class_weights_train)/ class_weights_train[i] #equalize number of background and signal event\n",
    "    weights_test[y_test == i] *= 1/(1-train_size) #increase test weight to compensate for sampling\n",
    "    \n",
    "print (\"Train : total weight sig\", weights_train[y_train == 1].sum())\n",
    "print (\"Train : total weight bkg\", weights_train[y_train == 0].sum())\n",
    "print (\"Test : total weight sig\", weights_test[y_test == 1].sum())\n",
    "print (\"Test : total weight bkg\", weights_test[y_test == 0].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxybCOi-rlIM"
   },
   "source": [
    "# Building a TensorFlow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Quickly take a look at weights\n",
    "print(X_train.shape)\n",
    "print(class_weights_train)\n",
    "print(weights_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how those shapes affect the network inputs - they _must_ match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train.shape[1],)), # input layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'), # 1st hiddden layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'), # 2nd hiddden layer\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\") # output layer\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "starting_time = time.time( )\n",
    "the_fit = model.fit(X_train, y_train.values, epochs=10,\n",
    "                     sample_weight=weights_train)\n",
    "#not using validation dataset here; keras does not support val weights\n",
    "#Solution: Define your own loss function that takes train and val loss! \n",
    "\n",
    "training_time = time.time( ) - starting_time\n",
    "print(\"Training time:\",training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how long this took compared to the BDT training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(the_fit.history['loss'],label=\"training loss\")\n",
    "plt.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model to make predicions!\n",
    "Evaluate the model based on predictions made with X_test $\\rightarrow$ y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test).ravel()\n",
    "y_pred_train = model.predict(X_train).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ROC curves and Area Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfB3mkkxW4CP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
    "from sklearn.utils import class_weight # to set class_weight=\"balanced\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr,tpr,_ = roc_curve(y_true=y_test, y_score=y_pred_test,sample_weight=weights_test)\n",
    "plt.plot(fpr, tpr, color='blue',lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auc_test = roc_auc_score(y_true=y_test, y_score=y_pred_test,sample_weight=weights_test)\n",
    "auc_train = roc_auc_score(y_true=y_train.values, y_score=y_pred_train,sample_weight=weights_train)\n",
    "print(\"auc test:\",auc_test)\n",
    "print (\"auc train:\",auc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIsMSGl-Kwql",
    "tags": []
   },
   "source": [
    "## Significance Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathrm{med}[Z_0|1] = \\sqrt{q_{0,A}} = \\sqrt{2+((s+b)\\ln(1+s/b)-s)}$\n",
    "\n",
    "**asimov significance [arXiv:1007.1727](https://arxiv.org/pdf/1007.1727.pdf) [Eq. 97]**\n",
    "\n",
    "Likelihood-based statistical test for significance. Need to esimate your sensitivity to MC. Running a toy MC thousands of times, should converge to 'truth'. Asimov is representative of number of sigmas in the gaus case.\n",
    "\n",
    "Essentially: For an observed number of signal events $s$, what is the significance $Z_0$ with which we would reject the $s = 0$ hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qubY3CMNKwql",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from math import log\n",
    "def amsasimov(s,b):\n",
    "        if b<=0 or s<=0:\n",
    "            return 0\n",
    "        try:\n",
    "            return sqrt(2*((s+b)*log(1+float(s)/b)-s))\n",
    "        except ValueError:\n",
    "            print(1+float(s)/b)\n",
    "            print (2*((s+b)*log(1+float(s)/b)-s))\n",
    "        #return s/sqrt(s+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqiOrCfyVYD4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from extra_functions import amsasimov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "int_pred_test_sig = [weights_test[(y_test ==1) & (y_pred_test > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "int_pred_test_bkg = [weights_test[(y_test ==0) & (y_pred_test > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "vamsasimov = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig,int_pred_test_bkg)]\n",
    "Z = max(vamsasimov)\n",
    "print(\"Z:\",Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1,num=50),vamsasimov, label='Significance (Z = {})'.format(np.round(Z,decimals=2)))\n",
    "\n",
    "\n",
    "plt.title(\"NN Significance\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Significance\")\n",
    "plt.legend()\n",
    "plt.savefig(\"Significance_xgb.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting NN Score for Signal and Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from extra_functions import compare_train_test\n",
    "compare_train_test(y_pred_train, y_train, y_pred_test, y_test, \n",
    "                   xlabel=\"NN Score\", title=\"NN\", \n",
    "                   weights_train=weights_train.values, weights_test=weights_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does overtraining look like?\n",
    "\n",
    "Recipe:\n",
    "1. Add More layers\n",
    "2. Add more nodes per layer\n",
    "3. Train on less data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Crazy Example\n",
    "N = len(X_train)\n",
    "n = int(N/1000)\n",
    "print(\"Using\",n,\"/\",N, \"events\")\n",
    "\n",
    "X_small = X_train[:n]\n",
    "y_small = y_train[:n]\n",
    "weights_small = weights_train[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is going to take a very long time (perhaps 10 minutes or so, depending on the CPU you landed on)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ot_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(X_small.shape[1],)), # input layer\n",
    "    tf.keras.layers.Dense(256, activation='relu'), # 1st hiddden layer\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "])\n",
    "ot_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "starting_time = time.time( )\n",
    "\n",
    "the_overfit = ot_model.fit(X_small, y_small.values, epochs=25 ,validation_data=(X_val, y_val))\n",
    "#Here, we are not using any sample weights. \n",
    "#This is just an example so we omits weights to show val loss\n",
    "\n",
    "training_time = time.time( ) - starting_time\n",
    "print(\"Training time:\",training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ot_y_pred_test = ot_model.predict(X_test).ravel()\n",
    "ot_y_pred_train = ot_model.predict(X_small).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, sharex=False, figsize=(15,5))\n",
    "\n",
    "axes[0].plot(the_overfit.history['loss'],label=\"training loss\")\n",
    "axes[0].plot(the_overfit.history['val_loss'],label=\"validation loss\")\n",
    "axes[0].legend(fontsize=15)\n",
    "\n",
    "axes[1].set_title(\"Overtrained NN\")\n",
    "compare_train_test(ot_y_pred_train, y_small, ot_y_pred_test, y_test, \n",
    "                   xlabel=\"NN Score\", title=\"Overtrained NN\", \n",
    "                   weights_train=weights_small.values, weights_test=weights_test.values)\n",
    "axes[1].legend(loc=\"upper center\",fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is over training? What has actually happened here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JKKJbnWX7Ql"
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQJuC2-1UkqP"
   },
   "source": [
    "1.   Improve NN AUC and significance by increasing the number of neurons, and layers, epochs, or by any other techniques (google) \n",
    "        - (beware of training time)\n",
    "        - Explore!\n",
    "        - Loss: [BCE, MSE,]\n",
    "        - Activations: [relu, leakyrelu, selu, tanh]\n",
    "2.   Draw NN score for signal and background, training and testing (see overtraining example, or BDT Notebook)\n",
    "        - feel free to look into *extra_functions.py*\n",
    "3.   Add Features more features and engineer aditional Features        \n",
    "___\n",
    "4.   Draw plot looking at significance for NN ( see BDT notebook), for the same features as BDT\n",
    "5.   Enable feature permutation importance (see BDT notebook)\n",
    "6.   Calculate the permutation importance with respect to significance and accuracy. See how Asimov significance changes as features shuffle.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Some help for BDT comparisons:\n",
    "y_pred_xgb = np.load(\"y_pred_xgb.npy\")\n",
    "y_pred_train_xgb = np.load(\"./y_pred_train_xgb.npy\")\n",
    "weights_train_xgb = np.load(\"./weights_train_xgb.npy\")\n",
    "y_test_xgb = np.load(\"./y_test_xgb.npy\")\n",
    "weights_test_xgb = np.load(\"./weights_test_xgb.npy\")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "xgb = xgboost.XGBClassifier({'nthread': 4})  # init model\n",
    "xgb.load_model('xgb.json')  # load data\n",
    "\n",
    "#The model and predictions from the BDT notebook are now in memory!\n",
    "#plt.bar(data.columns.values, xgb.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Unweighted Score Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_train_test(y_pred_train, y_train, y_pred_test, y_test, \n",
    "                   xlabel=\"NN Score\", title=\"NN\", \n",
    "                   weights_train=weights_train.values,\n",
    "                   weights_test=weights_test.values,density=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HEPML_HandsOn_NN_veryold.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
